{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c149796d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Load Successful!\n",
      "  book_theater_id   show_date  audience_count\n",
      "0      book_00001  2023-01-13              50\n",
      "1      book_00001  2023-01-14              64\n",
      "2      book_00001  2023-01-15              58\n",
      "3      book_00001  2023-01-16              44\n",
      "4      book_00001  2023-01-18              12\n"
     ]
    }
   ],
   "source": [
    "# !!! REPLACE 'YOUR/FULL/ABSOLUTE/PATH/TO/THE/FILE' WITH THE ACTUAL PATH !!!\n",
    "# Example: C:\\Users\\YourName\\KaggleProject\\dataset\\booknow_visits\\booknow_visits.csv\n",
    "# Example: /home/user/kaggle_data/dataset/booknow_visits/booknow_visits.csv\n",
    "\n",
    "df_test = pd.read_csv('D:/Projects/Kaggle Challenge/Cinema-Audiance/dataset/booknow_visits/booknow_visits.csv')\n",
    "print(\"Test Load Successful!\")\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a68a4fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1: Data Understanding and Preprocessing ---\n",
      "Loading all 8 files and applying initial date conversions...\n",
      "Successfully loaded: booknow_visits\n",
      "Successfully loaded: date_info\n",
      "Successfully loaded: movie_theater_id_relation\n",
      "Successfully loaded: booknow_booking\n",
      "Successfully loaded: booknow_theaters\n",
      "Successfully loaded: cinePOS_booking\n",
      "Successfully loaded: cinePOS_theaters\n",
      "Successfully loaded: sample_submission\n",
      "\n",
      "\n",
      "*** INSPECTION: BOOKNOW_VISITS ***\n",
      "--- HEAD ---\n",
      "  book_theater_id  show_date  audience_count\n",
      "0      book_00001 2023-01-13              50\n",
      "1      book_00001 2023-01-14              64\n",
      "2      book_00001 2023-01-15              58\n",
      "3      book_00001 2023-01-16              44\n",
      "4      book_00001 2023-01-18              12\n",
      "\n",
      "--- INFO ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 214046 entries, 0 to 214045\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   book_theater_id  214046 non-null  object        \n",
      " 1   show_date        214046 non-null  datetime64[ns]\n",
      " 2   audience_count   214046 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int64(1), object(1)\n",
      "memory usage: 4.9+ MB\n",
      "\n",
      "\n",
      "*** INSPECTION: DATE_INFO ***\n",
      "--- HEAD ---\n",
      "   show_date day_of_week\n",
      "0 2023-01-01      Sunday\n",
      "1 2023-01-02      Monday\n",
      "2 2023-01-03     Tuesday\n",
      "3 2023-01-04   Wednesday\n",
      "4 2023-01-05    Thursday\n",
      "\n",
      "--- INFO ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 547 entries, 0 to 546\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   show_date    547 non-null    datetime64[ns]\n",
      " 1   day_of_week  547 non-null    object        \n",
      "dtypes: datetime64[ns](1), object(1)\n",
      "memory usage: 8.7+ KB\n",
      "\n",
      "\n",
      "*** INSPECTION: MOVIE_THEATER_ID_RELATION ***\n",
      "--- HEAD ---\n",
      "  book_theater_id cine_theater_id\n",
      "0      book_00509   cinePOS_01261\n",
      "1      book_00063   cinePOS_02467\n",
      "2      book_00054   cinePOS_08923\n",
      "3      book_00094   cinePOS_02479\n",
      "4      book_00052   cinePOS_06750\n",
      "\n",
      "--- INFO ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 2 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   book_theater_id  150 non-null    object\n",
      " 1   cine_theater_id  150 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.5+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. BASE PATH DEFINITION (derived from successful test)\n",
    "BASE_PATH = 'D:/Projects/Kaggle Challenge/Cinema-Audiance/'\n",
    "PATH_TEMPLATE = BASE_PATH + 'dataset/{df_name}/{df_name}.csv'\n",
    "\n",
    "# 2. FILE NAMES\n",
    "FILE_NAMES = [\n",
    "    'booknow_visits',\n",
    "    'date_info',\n",
    "    'movie_theater_id_relation',\n",
    "    'booknow_booking',\n",
    "    'booknow_theaters',\n",
    "    'cinePOS_booking',\n",
    "    'cinePOS_theaters',\n",
    "    'sample_submission'\n",
    "]\n",
    "\n",
    "dfs = {}\n",
    "print(\"--- Phase 1: Data Understanding and Preprocessing ---\")\n",
    "print(\"Loading all 8 files and applying initial date conversions...\")\n",
    "\n",
    "# --- Data Loading Loop ---\n",
    "for df_name in FILE_NAMES:\n",
    "    file_path = PATH_TEMPLATE.format(df_name=df_name)\n",
    "    try:\n",
    "        # Load the file\n",
    "        dfs[df_name] = pd.read_csv(file_path)\n",
    "        print(f\"Successfully loaded: {df_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {df_name}: {e}. Skipping inspection for this file.\")\n",
    "        dfs[df_name] = pd.DataFrame()\n",
    "\n",
    "\n",
    "# --- Initial Cleaning: Date Conversion ---\n",
    "# Date columns to convert for main files\n",
    "date_cols = {\n",
    "    'booknow_visits': ['show_date'],\n",
    "    'date_info': ['show_date'],\n",
    "    'booknow_booking': ['show_datetime', 'booking_datetime'],\n",
    "    'cinePOS_booking': ['show_datetime', 'booking_datetime']\n",
    "}\n",
    "\n",
    "for df_name, cols in date_cols.items():\n",
    "    if df_name in dfs and not dfs[df_name].empty:\n",
    "        for col in cols:\n",
    "            if col in dfs[df_name].columns:\n",
    "                # Convert to datetime, coercing errors to NaT\n",
    "                dfs[df_name][col] = pd.to_datetime(dfs[df_name][col], errors='coerce')\n",
    "\n",
    "\n",
    "# --- Inspection of Critical Files ---\n",
    "critical_files = ['booknow_visits', 'date_info', 'movie_theater_id_relation']\n",
    "\n",
    "for df_name in critical_files:\n",
    "    print(f\"\\n\\n*** INSPECTION: {df_name.upper()} ***\")\n",
    "    df = dfs[df_name]\n",
    "    if not df.empty:\n",
    "        print(\"--- HEAD ---\")\n",
    "        print(df.head())\n",
    "        print(\"\\n--- INFO ---\")\n",
    "        df.info()\n",
    "    else:\n",
    "        print(f\"Warning: {df_name.upper()} was not loaded correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d72622c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2.1: Column Standardization ---\n",
      "Standardized 'book_theater_id' to 'theater_id' in booknow dataframes.\n",
      "\n",
      "--- Step 2.2: Merge Core Dataframes ---\n",
      "Merged booknow_visits with date_info.\n",
      "Merged with booknow_theaters metadata.\n",
      "\n",
      "--- Inspection of Base Training Data (df_train) ---\n",
      "Data Shape: (214046, 8)\n",
      "--- HEAD ---\n",
      "   theater_id  show_date  audience_count day_of_week theater_type  \\\n",
      "0  book_00001 2023-01-13              50      Friday        Drama   \n",
      "1  book_00001 2023-01-14              64    Saturday        Drama   \n",
      "2  book_00001 2023-01-15              58      Sunday        Drama   \n",
      "3  book_00001 2023-01-16              44      Monday        Drama   \n",
      "4  book_00001 2023-01-18              12   Wednesday        Drama   \n",
      "\n",
      "  theater_area  latitude  longitude  \n",
      "0     Area_002  23.00441  79.934515  \n",
      "1     Area_002  23.00441  79.934515  \n",
      "2     Area_002  23.00441  79.934515  \n",
      "3     Area_002  23.00441  79.934515  \n",
      "4     Area_002  23.00441  79.934515  \n",
      "\n",
      "--- INFO (Focusing on Merged Columns) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 214046 entries, 0 to 214045\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count   Dtype         \n",
      "---  ------          --------------   -----         \n",
      " 0   theater_id      214046 non-null  object        \n",
      " 1   show_date       214046 non-null  datetime64[ns]\n",
      " 2   audience_count  214046 non-null  int64         \n",
      " 3   day_of_week     214046 non-null  object        \n",
      " 4   theater_type    80150 non-null   object        \n",
      " 5   theater_area    80150 non-null   object        \n",
      " 6   latitude        80150 non-null   float64       \n",
      " 7   longitude       80150 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(2), int64(1), object(4)\n",
      "memory usage: 13.1+ MB\n",
      "\n",
      "Null Value Check after Merges:\n",
      "theater_id             0\n",
      "show_date              0\n",
      "audience_count         0\n",
      "day_of_week            0\n",
      "theater_type      133896\n",
      "theater_area      133896\n",
      "latitude          133896\n",
      "longitude         133896\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Assuming all dataframes (dfs) are successfully loaded from the previous step\n",
    "\n",
    "print(\"\\n--- Step 2.1: Column Standardization ---\")\n",
    "# Standardize theater ID column names for BookNow data\n",
    "dfs['booknow_visits'].rename(columns={'book_theater_id': 'theater_id'}, inplace=True)\n",
    "dfs['booknow_theaters'].rename(columns={'book_theater_id': 'theater_id'}, inplace=True)\n",
    "dfs['movie_theater_id_relation'].rename(columns={'book_theater_id': 'theater_id'}, inplace=True)\n",
    "\n",
    "print(\"Standardized 'book_theater_id' to 'theater_id' in booknow dataframes.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Step 2.2: Merge Core Dataframes ---\")\n",
    "\n",
    "# 1. Merge Target with Calendar Features (booknow_visits + date_info)\n",
    "df_train = pd.merge(\n",
    "    dfs['booknow_visits'], \n",
    "    dfs['date_info'], \n",
    "    on='show_date', \n",
    "    how='left'\n",
    ")\n",
    "print(\"Merged booknow_visits with date_info.\")\n",
    "\n",
    "# 2. Merge with BookNow Theater Metadata (df_train + booknow_theaters)\n",
    "df_train = pd.merge(\n",
    "    df_train, \n",
    "    dfs['booknow_theaters'], \n",
    "    on='theater_id', \n",
    "    how='left'\n",
    ")\n",
    "print(\"Merged with booknow_theaters metadata.\")\n",
    "\n",
    "print(\"\\n--- Inspection of Base Training Data (df_train) ---\")\n",
    "print(\"Data Shape:\", df_train.shape)\n",
    "print(\"--- HEAD ---\")\n",
    "print(df_train.head())\n",
    "print(\"\\n--- INFO (Focusing on Merged Columns) ---\")\n",
    "df_train.info()\n",
    "\n",
    "# Check for nulls introduced by the merge (theater metadata)\n",
    "print(\"\\nNull Value Check after Merges:\")\n",
    "print(df_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d10b63d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3.1: Aggregate BookNow Bookings ---\n",
      "BookNow aggregation completed. Shape: (21590, 3)\n",
      "\n",
      "--- Step 3.2: Aggregate CinePOS Bookings ---\n",
      "CinePOS aggregation and mapping completed. Shape: (15789, 3)\n",
      "\n",
      "--- Step 3.3: Merge Aggregated Bookings into df_train ---\n",
      "Merged all booking features. Nulls imputed with 0.\n",
      "\n",
      "--- Inspection of df_train with New Booking Features ---\n",
      "Data Shape: (214046, 10)\n",
      "--- HEAD with New Columns ---\n",
      "   theater_id  show_date  audience_count  booknow_tickets  cinepos_tickets\n",
      "0  book_00001 2023-01-13              50              0.0              0.0\n",
      "1  book_00001 2023-01-14              64              0.0              0.0\n",
      "2  book_00001 2023-01-15              58              0.0              0.0\n",
      "3  book_00001 2023-01-16              44              0.0              0.0\n",
      "4  book_00001 2023-01-18              12              0.0              0.0\n",
      "5  book_00001 2023-01-19              18              0.0              0.0\n",
      "6  book_00001 2023-01-20              62              0.0              0.0\n",
      "7  book_00001 2023-01-21              42              0.0              0.0\n",
      "8  book_00001 2023-01-22              36              0.0              0.0\n",
      "9  book_00001 2023-01-23              52              0.0              0.0\n",
      "\n",
      "--- Null Value Check (Full) ---\n",
      "theater_id              0\n",
      "show_date               0\n",
      "audience_count          0\n",
      "day_of_week             0\n",
      "theater_type       133896\n",
      "theater_area       133896\n",
      "latitude           133896\n",
      "longitude          133896\n",
      "booknow_tickets         0\n",
      "cinepos_tickets         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sajit\\AppData\\Local\\Temp\\ipykernel_23860\\996021168.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_train['booknow_tickets'].fillna(0, inplace=True)\n",
      "C:\\Users\\sajit\\AppData\\Local\\Temp\\ipykernel_23860\\996021168.py:46: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_train['cinepos_tickets'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Assuming all dataframes (dfs) are successfully loaded and standardized from the previous steps\n",
    "\n",
    "print(\"\\n--- Step 3.1: Aggregate BookNow Bookings ---\")\n",
    "\n",
    "# Extract the show_date from show_datetime\n",
    "dfs['booknow_booking']['show_date'] = dfs['booknow_booking']['show_datetime'].dt.normalize()\n",
    "dfs['booknow_booking'].rename(columns={'book_theater_id': 'theater_id', 'tickets_booked': 'booknow_tickets'}, inplace=True)\n",
    "\n",
    "# Group by theater and date, summing the booked tickets\n",
    "df_booknow_agg = dfs['booknow_booking'].groupby(['theater_id', 'show_date'])['booknow_tickets'].sum().reset_index()\n",
    "\n",
    "print(f\"BookNow aggregation completed. Shape: {df_booknow_agg.shape}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Step 3.2: Aggregate CinePOS Bookings ---\")\n",
    "\n",
    "# 1. Prepare CinePOS booking data (extract date and rename tickets column)\n",
    "dfs['cinePOS_booking']['show_date'] = dfs['cinePOS_booking']['show_datetime'].dt.normalize()\n",
    "dfs['cinePOS_booking'].rename(columns={'tickets_sold': 'cinepos_tickets'}, inplace=True)\n",
    "\n",
    "# 2. Merge CinePOS bookings with the relation table to get the 'theater_id' (BookNow ID)\n",
    "df_cinepos_merged = pd.merge(\n",
    "    dfs['cinePOS_booking'],\n",
    "    dfs['movie_theater_id_relation'],\n",
    "    on='cine_theater_id',\n",
    "    how='left'\n",
    ")\n",
    "# Note: The 'theater_id' column already exists from the standardization step (book_theater_id was renamed to theater_id)\n",
    "\n",
    "# 3. Group by theater (BookNow ID) and date, summing the sold tickets\n",
    "df_cinepos_agg = df_cinepos_merged.groupby(['theater_id', 'show_date'])['cinepos_tickets'].sum().reset_index()\n",
    "\n",
    "print(f\"CinePOS aggregation and mapping completed. Shape: {df_cinepos_agg.shape}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Step 3.3: Merge Aggregated Bookings into df_train ---\")\n",
    "\n",
    "# Merge aggregated BookNow tickets\n",
    "df_train = pd.merge(df_train, df_booknow_agg, on=['theater_id', 'show_date'], how='left')\n",
    "\n",
    "# Merge aggregated CinePOS tickets\n",
    "df_train = pd.merge(df_train, df_cinepos_agg, on=['theater_id', 'show_date'], how='left')\n",
    "\n",
    "# Impute NaN booking features with 0 (No booking recorded = 0 tickets booked)\n",
    "df_train['booknow_tickets'].fillna(0, inplace=True)\n",
    "df_train['cinepos_tickets'].fillna(0, inplace=True)\n",
    "\n",
    "print(\"Merged all booking features. Nulls imputed with 0.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Inspection of df_train with New Booking Features ---\")\n",
    "print(\"Data Shape:\", df_train.shape)\n",
    "print(\"--- HEAD with New Columns ---\")\n",
    "print(df_train[['theater_id', 'show_date', 'audience_count', 'booknow_tickets', 'cinepos_tickets']].head(10))\n",
    "print(\"\\n--- Null Value Check (Full) ---\")\n",
    "print(df_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fad052ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4.1: Sort Data for Time-Series Features ---\n",
      "Data sorted by theater_id and show_date.\n",
      "\n",
      "--- Step 4.2: Creating Lag Features (T-7 and T-14) ---\n",
      "Created lag_7 and lag_14.\n",
      "\n",
      "--- Step 4.3: Creating Rolling Mean Features (Window 7) ---\n",
      "Created rolling_mean_7.\n",
      "\n",
      "--- Inspection of df_train with New Time-Series Features ---\n",
      "Data Shape: (214046, 13)\n",
      "--- HEAD with New Columns (Showing a theater's time series) ---\n",
      "    show_date  audience_count  lag_7  lag_14  rolling_mean_7\n",
      "0  2023-01-13              50    NaN     NaN             NaN\n",
      "1  2023-01-14              64    NaN     NaN       50.000000\n",
      "2  2023-01-15              58    NaN     NaN       57.000000\n",
      "3  2023-01-16              44    NaN     NaN       57.333333\n",
      "4  2023-01-18              12    NaN     NaN       54.000000\n",
      "5  2023-01-19              18    NaN     NaN       45.600000\n",
      "6  2023-01-20              62    NaN     NaN       41.000000\n",
      "7  2023-01-21              42   50.0     NaN       44.000000\n",
      "8  2023-01-22              36   64.0     NaN       42.857143\n",
      "9  2023-01-23              52   58.0     NaN       38.857143\n",
      "10 2023-01-25              42   44.0     NaN       38.000000\n",
      "11 2023-01-26              22   12.0     NaN       37.714286\n",
      "12 2023-01-27              48   18.0     NaN       39.142857\n",
      "13 2023-01-28              42   62.0     NaN       43.428571\n",
      "14 2023-01-29              52   42.0    50.0       40.571429\n",
      "15 2023-01-30              12   36.0    64.0       42.000000\n",
      "16 2023-02-03              36   52.0    58.0       38.571429\n",
      "17 2023-02-04              24   42.0    44.0       36.285714\n",
      "18 2023-02-05              90   22.0    12.0       33.714286\n",
      "19 2023-02-06              30   48.0    18.0       43.428571\n",
      "\n",
      "--- Null Value Check (Full) ---\n",
      "theater_id              0\n",
      "show_date               0\n",
      "audience_count          0\n",
      "day_of_week             0\n",
      "theater_type       133896\n",
      "theater_area       133896\n",
      "latitude           133896\n",
      "longitude          133896\n",
      "booknow_tickets         0\n",
      "cinepos_tickets         0\n",
      "lag_7                5771\n",
      "lag_14              11539\n",
      "rolling_mean_7        826\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Step 4.1: Sort Data for Time-Series Features ---\")\n",
    "# Sorting is essential for calculating correct lags and rolling statistics\n",
    "df_train.sort_values(by=['theater_id', 'show_date'], inplace=True)\n",
    "print(\"Data sorted by theater_id and show_date.\")\n",
    "\n",
    "# --- Step 4.2: Create Lag Features (Past Audience Counts) ---\n",
    "print(\"\\n--- Step 4.2: Creating Lag Features (T-7 and T-14) ---\")\n",
    "# Lagging must be done within each group (per theater_id)\n",
    "df_train['lag_7'] = df_train.groupby('theater_id')['audience_count'].shift(7)\n",
    "df_train['lag_14'] = df_train.groupby('theater_id')['audience_count'].shift(14)\n",
    "print(\"Created lag_7 and lag_14.\")\n",
    "\n",
    "# --- Step 4.3: Create Rolling Mean Features (Weekly Trend) ---\n",
    "print(\"\\n--- Step 4.3: Creating Rolling Mean Features (Window 7) ---\")\n",
    "# Rolling Mean (past 7 days, excluding current day)\n",
    "# min_periods=1 allows calculation even with less than 7 data points (important for early dates)\n",
    "df_train['rolling_mean_7'] = df_train.groupby('theater_id')['audience_count'].transform(\n",
    "    lambda x: x.shift(1).rolling(window=7, min_periods=1).mean()\n",
    ")\n",
    "print(\"Created rolling_mean_7.\")\n",
    "\n",
    "# Fill NaNs created by lags/rolling mean at the start of each theater's series\n",
    "# Lags and rolling means at the start of the time series are legitimately NaN. \n",
    "# We will leave these as NaNs for now, as modern models handle them well, but for simplicity, \n",
    "# let's fill them with a specific value or the mean later if a model requires it.\n",
    "\n",
    "print(\"\\n--- Inspection of df_train with New Time-Series Features ---\")\n",
    "print(\"Data Shape:\", df_train.shape)\n",
    "print(\"--- HEAD with New Columns (Showing a theater's time series) ---\")\n",
    "# Show the first 20 rows of the first theater to see lags populate\n",
    "print(df_train[df_train['theater_id'] == 'book_00001'].head(20)[['show_date', 'audience_count', 'lag_7', 'lag_14', 'rolling_mean_7']])\n",
    "print(\"\\n--- Null Value Check (Full) ---\")\n",
    "print(df_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d31a9b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 5.1: Prepare Prediction Dataframe (df_test) ---\n",
      "Split ID and extracted 'theater_id' and 'show_date'.\n",
      "Merged calendar and theater metadata into df_test.\n",
      "Merged aggregated booking features.\n",
      "Calculating Lags and Rolling Means on the combined dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sajit\\AppData\\Local\\Temp\\ipykernel_23860\\2819814864.py:34: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_test['booknow_tickets'].fillna(0, inplace=True)\n",
      "C:\\Users\\sajit\\AppData\\Local\\Temp\\ipykernel_23860\\2819814864.py:35: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_test['cinepos_tickets'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged time-series features into df_test.\n",
      "\n",
      "--- Inspection of Prediction Data (df_test) ---\n",
      "Data Shape: (38062, 12)\n",
      "--- HEAD with All Features ---\n",
      "   theater_id  show_date day_of_week theater_type theater_area  latitude  \\\n",
      "0  book_00001 2024-03-01      Friday        Drama     Area_002  23.00441   \n",
      "1  book_00001 2024-03-02    Saturday        Drama     Area_002  23.00441   \n",
      "2  book_00001 2024-03-03      Sunday        Drama     Area_002  23.00441   \n",
      "3  book_00001 2024-03-04      Monday        Drama     Area_002  23.00441   \n",
      "4  book_00001 2024-03-06   Wednesday        Drama     Area_002  23.00441   \n",
      "\n",
      "   longitude  booknow_tickets  cinepos_tickets  lag_7  lag_14  rolling_mean_7  \n",
      "0  79.934515              0.0              0.0   28.0    34.0       38.857143  \n",
      "1  79.934515              0.0              0.0   50.0    30.0       40.666667  \n",
      "2  79.934515              0.0              0.0   22.0    32.0       38.800000  \n",
      "3  79.934515              0.0              0.0   80.0    78.0       43.000000  \n",
      "4  79.934515              0.0              0.0   54.0    44.0       30.666667  \n",
      "\n",
      "--- Null Value Check (Key Features) ---\n",
      "theater_type      23456\n",
      "lag_7             32316\n",
      "lag_14            26602\n",
      "rolling_mean_7    32305\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 5.1: Prepare Prediction Dataframe (df_test) ---\")\n",
    "\n",
    "# 1. Load and Split Submission ID (CORRECTED LOGIC)\n",
    "df_test = dfs['sample_submission'].copy()\n",
    "\n",
    "# Split the ID into 3 parts: [prefix, id_number, date]\n",
    "# Example: 'book_00001_2024-03-01' -> ['book', '00001', '2024-03-01'] (This still doesn't work if theater_id has multiple parts like 'book_00001').\n",
    "# Let's assume the ID format is {theater_id}_{date} and the theater_id itself contains underscores.\n",
    "\n",
    "# A more robust solution: Split only on the last underscore (to separate the date)\n",
    "df_test['theater_id'] = df_test['ID'].str.rsplit('_', n=1).str[0] # Everything before the last '_'\n",
    "df_test['show_date'] = df_test['ID'].str.rsplit('_', n=1).str[1] # Everything after the last '_'\n",
    "\n",
    "df_test['show_date'] = pd.to_datetime(df_test['show_date'], format='%Y-%m-%d')\n",
    "df_test.drop(columns=['ID', 'audience_count'], inplace=True) \n",
    "\n",
    "# Ensure the theater_id column name matches the standardized name\n",
    "# We already renamed 'book_theater_id' to 'theater_id' in previous steps, but let's confirm.\n",
    "# The `theater_id` from the split is the full 'book_00001' string.\n",
    "\n",
    "print(\"Split ID and extracted 'theater_id' and 'show_date'.\")\n",
    "\n",
    "\n",
    "# 2. Merge Calendar and Theater Metadata\n",
    "df_test = pd.merge(df_test, dfs['date_info'], on='show_date', how='left')\n",
    "df_test = pd.merge(df_test, dfs['booknow_theaters'], on='theater_id', how='left')\n",
    "print(\"Merged calendar and theater metadata into df_test.\")\n",
    "\n",
    "# 3. Merge Booking Features (These will be sparse/mostly zero, but required)\n",
    "# Note: We must use the AGGREGATED booking dataframes created in Step 3\n",
    "# Ensure df_booknow_agg and df_cinepos_agg are available in your environment!\n",
    "df_test = pd.merge(df_test, df_booknow_agg, on=['theater_id', 'show_date'], how='left')\n",
    "df_test = pd.merge(df_test, df_cinepos_agg, on=['theater_id', 'show_date'], how='left')\n",
    "df_test['booknow_tickets'].fillna(0, inplace=True)\n",
    "df_test['cinepos_tickets'].fillna(0, inplace=True)\n",
    "print(\"Merged aggregated booking features.\")\n",
    "\n",
    "# 4. Prepare for Lag Calculation\n",
    "# Select necessary columns for the full time series\n",
    "df_full = pd.concat([\n",
    "    df_train[['theater_id', 'show_date', 'audience_count']],\n",
    "    df_test[['theater_id', 'show_date']]\n",
    "], ignore_index=True)\n",
    "\n",
    "df_full.sort_values(by=['theater_id', 'show_date'], inplace=True)\n",
    "\n",
    "# 5. Calculate Lags and Rolling Means on the full time series\n",
    "print(\"Calculating Lags and Rolling Means on the combined dataset...\")\n",
    "df_full['lag_7'] = df_full.groupby('theater_id')['audience_count'].shift(7)\n",
    "df_full['lag_14'] = df_full.groupby('theater_id')['audience_count'].shift(14)\n",
    "df_full['rolling_mean_7'] = df_full.groupby('theater_id')['audience_count'].transform(\n",
    "    lambda x: x.shift(1).rolling(window=7, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "# 6. Merge the calculated features back into df_test\n",
    "# Filter df_full to only contain the prediction dates\n",
    "df_full_test = df_full[df_full['show_date'].isin(df_test['show_date']) & df_full['audience_count'].isnull()]\n",
    "df_full_test = df_full_test[['theater_id', 'show_date', 'lag_7', 'lag_14', 'rolling_mean_7']]\n",
    "\n",
    "# Final merge of the time-series features into df_test\n",
    "df_test = pd.merge(df_test, df_full_test, on=['theater_id', 'show_date'], how='left')\n",
    "print(\"Merged time-series features into df_test.\")\n",
    "\n",
    "print(\"\\n--- Inspection of Prediction Data (df_test) ---\")\n",
    "print(\"Data Shape:\", df_test.shape)\n",
    "print(\"--- HEAD with All Features ---\")\n",
    "print(df_test.head())\n",
    "print(\"\\n--- Null Value Check (Key Features) ---\")\n",
    "print(df_test[['theater_type', 'lag_7', 'lag_14', 'rolling_mean_7']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7434e437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 6.1: Final Feature Preprocessing ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sajit\\AppData\\Local\\Temp\\ipykernel_23860\\3253639197.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_combined['latitude'].fillna(MEDIAN_LAT, inplace=True)\n",
      "C:\\Users\\sajit\\AppData\\Local\\Temp\\ipykernel_23860\\3253639197.py:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_combined['longitude'].fillna(MEDIAN_LON, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed all feature preprocessing (Encoding and Imputation).\n",
      "\n",
      "--- Step 6.3: Model Training (LightGBM) ---\n",
      "LightGBM Model Training Complete.\n",
      "\n",
      "--- Step 6.4: Prediction ---\n",
      "Predictions generated and assigned to df_test_final.\n",
      "\n",
      "--- Step 6.5: Create Submission File ---\n",
      "\n",
      "--- Final Submission Dataframe Head ---\n",
      "                      ID  audience_count\n",
      "0  book_00001_2024-03-01               0\n",
      "1  book_00001_2024-03-02               0\n",
      "2  book_00001_2024-03-03               0\n",
      "3  book_00001_2024-03-04               0\n",
      "4  book_00001_2024-03-06               0\n",
      "Submission file ready with 38062 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sajit\\AppData\\Local\\Temp\\ipykernel_23860\\3253639197.py:99: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_submission['audience_count'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "# --- 6.1: Final Feature Preprocessing ---\n",
    "print(\"\\n--- Step 6.1: Final Feature Preprocessing ---\")\n",
    "\n",
    "# Combine train and test for consistent encoding/imputation\n",
    "# Ensure dfs['sample_submission'] is available\n",
    "df_combined = pd.concat([df_train.drop(columns=['audience_count']), df_test], ignore_index=True)\n",
    "\n",
    "# 1. Handle Categorical Features (Label Encoding)\n",
    "CAT_COLS = ['theater_id', 'day_of_week', 'theater_type', 'theater_area']\n",
    "for col in CAT_COLS:\n",
    "    if col in ['theater_type', 'theater_area']:\n",
    "        df_combined[col] = df_combined[col].fillna('Missing')\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    df_combined[col] = le.fit_transform(df_combined[col].astype(str))\n",
    "    # print(f\"Label Encoded: {col}\") # Removed for cleaner output\n",
    "\n",
    "# 2. Handle Numerical Missing Values (Imputation)\n",
    "LAG_COLS = ['lag_7', 'lag_14', 'rolling_mean_7']\n",
    "for col in LAG_COLS:\n",
    "    df_combined[col] = df_combined[col].fillna(0)\n",
    "\n",
    "MEDIAN_LAT = df_train['latitude'].median()\n",
    "MEDIAN_LON = df_train['longitude'].median()\n",
    "df_combined['latitude'].fillna(MEDIAN_LAT, inplace=True)\n",
    "df_combined['longitude'].fillna(MEDIAN_LON, inplace=True)\n",
    "print(\"Completed all feature preprocessing (Encoding and Imputation).\")\n",
    "\n",
    "# --- Split back into Training and Testing sets ---\n",
    "df_train_final = df_combined.iloc[:len(df_train)].copy()\n",
    "df_train_final['audience_count'] = df_train['audience_count']\n",
    "df_test_final = df_combined.iloc[len(df_train):].copy() # THIS IS THE DATAFRAME FOR PREDICTION INPUT\n",
    "\n",
    "# --- 6.2: Define Features and Target ---\n",
    "FEATURES = [col for col in df_train_final.columns if col not in ['show_date', 'audience_count']]\n",
    "TARGET = 'audience_count'\n",
    "\n",
    "X = df_train_final[FEATURES]\n",
    "y = df_train_final[TARGET]\n",
    "X_test = df_test_final[FEATURES] # THIS IS THE FEATURE SET FOR PREDICTION\n",
    "\n",
    "\n",
    "# --- 6.3: Model Training (LightGBM) ---\n",
    "print(\"\\n--- Step 6.3: Model Training (LightGBM) ---\")\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "model = lgb.LGBMRegressor(**params)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"LightGBM Model Training Complete.\")\n",
    "\n",
    "\n",
    "# --- 6.4: Prediction ---\n",
    "print(\"\\n--- Step 6.4: Prediction ---\")\n",
    "\n",
    "# Generate predictions\n",
    "predictions = model.predict(X_test)\n",
    "predictions[predictions < 0] = 0\n",
    "\n",
    "# !!! CORRECTION HERE: Assign predictions to df_test_final !!!\n",
    "df_test_final['audience_count'] = predictions.round().astype(int)\n",
    "print(\"Predictions generated and assigned to df_test_final.\")\n",
    "\n",
    "\n",
    "# --- 6.5: Create Submission File ---\n",
    "print(\"\\n--- Step 6.5: Create Submission File ---\")\n",
    "\n",
    "df_submission = dfs['sample_submission'].copy()\n",
    "\n",
    "# 1. Recreate the ID in df_test_final\n",
    "df_test_final['ID'] = df_test_final['theater_id'].astype(str) + '_' + df_test_final['show_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# 2. Merge predictions back to the original submission format, preserving the exact ID order\n",
    "df_submission = pd.merge(\n",
    "    df_submission[['ID']], # Only keep the 'ID' column from original submission\n",
    "    df_test_final[['ID', 'audience_count']], # Now df_test_final correctly has 'audience_count'\n",
    "    on='ID', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Final formatting\n",
    "df_submission['audience_count'].fillna(0, inplace=True)\n",
    "df_submission['audience_count'] = df_submission['audience_count'].round().astype(int)\n",
    "\n",
    "# Final check\n",
    "print(\"\\n--- Final Submission Dataframe Head ---\")\n",
    "print(df_submission.head())\n",
    "print(f\"Submission file ready with {len(df_submission)} entries.\")\n",
    "\n",
    "# df_submission.to_csv('lgbm_submission_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1093b9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 7.1: Preprocessing Booking Data for Lead Time (Final Robust Fix) ---\n",
      "Combined and cleaned advance booking data. Total records: 1464940\n",
      "\n",
      "--- Step 7.2: Defining Lead Time Bins and Aggregating ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sajit\\AppData\\Local\\Temp\\ipykernel_23860\\1844710895.py:69: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df_adv_agg = df_bookings.groupby(['theater_id', 'show_date', 'adv_bin'])['tickets'].sum().reset_index()\n",
      "C:\\Users\\sajit\\AppData\\Local\\Temp\\ipykernel_23860\\1844710895.py:72: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  df_adv_pivot = df_adv_agg.pivot_table(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivot table created with 130707 unique date/theater combinations.\n",
      "\n",
      "--- Step 7.3: Merging New Features (Type Coercion Fix) ---\n",
      "Advance booking features merged. Train shape: (214046, 17), Test shape: (38062, 18)\n",
      "\n",
      "--- Inspection of Final Features in df_test_final ---\n",
      "   adv_1d  adv_3d  adv_7d  adv_total  theater_id  show_date\n",
      "0     0.0     0.0     0.0        0.0           0 2024-03-01\n",
      "1     0.0     0.0     0.0        0.0           0 2024-03-02\n",
      "2     0.0     0.0     0.0        0.0           0 2024-03-03\n",
      "3     0.0     0.0     0.0        0.0           0 2024-03-04\n",
      "4     0.0     0.0     0.0        0.0           0 2024-03-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sajit\\AppData\\Local\\Temp\\ipykernel_23860\\1844710895.py:104: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_train_final[col].fillna(0, inplace=True)\n",
      "C:\\Users\\sajit\\AppData\\Local\\Temp\\ipykernel_23860\\1844710895.py:105: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_test_final[col].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the dataframes we are working with\n",
    "df_booknow_base = dfs['booknow_booking'].copy()\n",
    "df_cinepos_base = dfs['cinePOS_booking'].copy()\n",
    "\n",
    "print(\"\\n--- Step 7.1: Preprocessing Booking Data for Lead Time (Final Robust Fix) ---\")\n",
    "\n",
    "# 1. Prepare BookNow Booking Data\n",
    "# The original ticket column name is 'tickets_booked'\n",
    "df_booknow_adv = df_booknow_base.rename(\n",
    "    columns={'tickets_booked': 'tickets', 'book_theater_id': 'theater_id'}, \n",
    "    errors='ignore' # Ignore if 'tickets_booked' is not there\n",
    ")\n",
    "# Check if the name applied in Step 3 ('booknow_tickets') is present and use it\n",
    "if 'booknow_tickets' in df_booknow_adv.columns:\n",
    "    df_booknow_adv.rename(columns={'booknow_tickets': 'tickets'}, inplace=True)\n",
    "\n",
    "# 2. Prepare CinePOS Booking Data\n",
    "# The original ticket column name is 'tickets_sold'\n",
    "df_cinepos_adv = df_cinepos_base.rename(\n",
    "    columns={'tickets_sold': 'tickets'}, \n",
    "    errors='ignore',\n",
    "    inplace=False\n",
    ")\n",
    "\n",
    "# Check if the name applied in Step 3 ('cinepos_tickets') is present and use it\n",
    "if 'cinepos_tickets' in df_cinepos_adv.columns:\n",
    "    df_cinepos_adv.rename(columns={'cinepos_tickets': 'tickets'}, inplace=True)\n",
    "\n",
    "\n",
    "# Merge CinePOS data with the relation table to get 'theater_id'\n",
    "df_cinepos_adv = pd.merge(\n",
    "    df_cinepos_adv,\n",
    "    dfs['movie_theater_id_relation'],\n",
    "    on='cine_theater_id',\n",
    "    how='left'\n",
    ").drop(columns=['cine_theater_id'])\n",
    "\n",
    "# Combine both booking dataframes, NOW both must have 'tickets' column\n",
    "df_bookings = pd.concat([\n",
    "    df_booknow_adv[['theater_id', 'show_datetime', 'booking_datetime', 'tickets']],\n",
    "    df_cinepos_adv[['theater_id', 'show_datetime', 'booking_datetime', 'tickets']]\n",
    "], ignore_index=True)\n",
    "\n",
    "# The rest of the code is correct and relies on 'df_bookings'\n",
    "# ... (Continuing with the rest of Step 7.1, 7.2, and 7.3)\n",
    "\n",
    "# Calculate Lead Time (in days)\n",
    "df_bookings['lead_days'] = (df_bookings['show_datetime'] - df_bookings['booking_datetime']).dt.days\n",
    "\n",
    "# Filter out tickets booked *after* the show and on the day of the show (lead_days >= 1)\n",
    "df_bookings = df_bookings[df_bookings['lead_days'] >= 1].copy()\n",
    "df_bookings['show_date'] = df_bookings['show_datetime'].dt.normalize()\n",
    "\n",
    "print(f\"Combined and cleaned advance booking data. Total records: {len(df_bookings)}\")\n",
    "\n",
    "\n",
    "# --- Step 7.2: Define Lead Time Bins and Aggregating ---\n",
    "print(\"\\n--- Step 7.2: Defining Lead Time Bins and Aggregating ---\")\n",
    "\n",
    "# Define the lead time bins\n",
    "bins = [1, 2, 4, 8, df_bookings['lead_days'].max() + 1]\n",
    "labels = ['adv_1d', 'adv_2_3d', 'adv_4_7d', 'adv_total']\n",
    "\n",
    "df_bookings['adv_bin'] = pd.cut(df_bookings['lead_days'], bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "\n",
    "# Aggregate advance bookings by show date, theater, and lead time bin\n",
    "df_adv_agg = df_bookings.groupby(['theater_id', 'show_date', 'adv_bin'])['tickets'].sum().reset_index()\n",
    "\n",
    "# Pivot the table to get one column per advance booking bin\n",
    "df_adv_pivot = df_adv_agg.pivot_table(\n",
    "    index=['theater_id', 'show_date'], \n",
    "    columns='adv_bin', \n",
    "    values='tickets', \n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "# Rename columns to match the plan\n",
    "df_adv_pivot.rename(columns={'adv_2_3d': 'adv_3d', 'adv_4_7d': 'adv_7d'}, inplace=True)\n",
    "\n",
    "print(f\"Pivot table created with {len(df_adv_pivot)} unique date/theater combinations.\")\n",
    "\n",
    "\n",
    "# Assuming df_adv_pivot and all final dataframes are available from the previous run\n",
    "\n",
    "print(\"\\n--- Step 7.3: Merging New Features (Type Coercion Fix) ---\")\n",
    "\n",
    "# --- Fix: Coerce theater_id to string (object) for merge ---\n",
    "# The theater_id in df_adv_pivot is a string. We must match that type.\n",
    "df_train_final['theater_id'] = df_train_final['theater_id'].astype(str)\n",
    "df_test_final['theater_id'] = df_test_final['theater_id'].astype(str)\n",
    "\n",
    "# --- Perform Merge ---\n",
    "# Merge into df_train_final\n",
    "df_train_final = pd.merge(df_train_final, df_adv_pivot, on=['theater_id', 'show_date'], how='left')\n",
    "\n",
    "# Merge into df_test_final\n",
    "df_test_final = pd.merge(df_test_final, df_adv_pivot, on=['theater_id', 'show_date'], how='left')\n",
    "\n",
    "# Fill NaNs created by the merge with 0 (No advance booking recorded)\n",
    "ADV_COLS = ['adv_1d', 'adv_3d', 'adv_7d', 'adv_total']\n",
    "for col in ADV_COLS:\n",
    "    df_train_final[col].fillna(0, inplace=True)\n",
    "    df_test_final[col].fillna(0, inplace=True)\n",
    "\n",
    "# Re-convert theater_id back to integer after merge for LGBM model\n",
    "df_train_final['theater_id'] = df_train_final['theater_id'].astype(int)\n",
    "df_test_final['theater_id'] = df_test_final['theater_id'].astype(int)\n",
    "\n",
    "\n",
    "print(f\"Advance booking features merged. Train shape: {df_train_final.shape}, Test shape: {df_test_final.shape}\")\n",
    "print(\"\\n--- Inspection of Final Features in df_test_final ---\")\n",
    "print(df_test_final[ADV_COLS + ['theater_id', 'show_date']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82dfe1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 8.1: Features Defined ---\n",
      "Training Model with 15 features. New features include: adv_1d, adv_3d, adv_7d, adv_total.\n",
      "\n",
      "--- Step 8.2: Model Training (LightGBM) ---\n",
      "LightGBM Model Training Complete.\n",
      "\n",
      "--- Step 8.3: Prediction and Submission ---\n",
      "Predictions generated.\n",
      "\n",
      "--- Final Submission Dataframe Head (Improved Predictions) ---\n",
      "                      ID  audience_count\n",
      "0  book_00001_2024-03-01               0\n",
      "1  book_00001_2024-03-02               0\n",
      "2  book_00001_2024-03-03               0\n",
      "3  book_00001_2024-03-04               0\n",
      "4  book_00001_2024-03-06               0\n",
      "Submission file ready with 38062 entries. This version should perform better due to advanced booking features!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sajit\\AppData\\Local\\Temp\\ipykernel_23860\\4130192245.py:65: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_submission['audience_count'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "# --- 8.1: Define Features and Target (Adding the new ADV_COLS) ---\n",
    "# Ensure the final dataframes are using the correct integer types for the model\n",
    "df_train_final['theater_id'] = df_train_final['theater_id'].astype(int)\n",
    "df_test_final['theater_id'] = df_test_final['theater_id'].astype(int)\n",
    "\n",
    "FEATURES = [col for col in df_train_final.columns if col not in ['show_date', 'audience_count']]\n",
    "TARGET = 'audience_count'\n",
    "\n",
    "X = df_train_final[FEATURES]\n",
    "y = df_train_final[TARGET]\n",
    "X_test = df_test_final[FEATURES]\n",
    "\n",
    "print(f\"\\n--- Step 8.1: Features Defined ---\")\n",
    "print(f\"Training Model with {len(FEATURES)} features. New features include: adv_1d, adv_3d, adv_7d, adv_total.\")\n",
    "\n",
    "\n",
    "# --- 8.2: Model Training (LightGBM) ---\n",
    "print(\"\\n--- Step 8.2: Model Training (LightGBM) ---\")\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "model = lgb.LGBMRegressor(**params)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"LightGBM Model Training Complete.\")\n",
    "\n",
    "\n",
    "# --- 8.3: Prediction and Submission ---\n",
    "print(\"\\n--- Step 8.3: Prediction and Submission ---\")\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "predictions[predictions < 0] = 0\n",
    "\n",
    "df_test_final['audience_count'] = predictions.round().astype(int)\n",
    "print(\"Predictions generated.\")\n",
    "\n",
    "# Create Submission File\n",
    "df_submission = dfs['sample_submission'].copy()\n",
    "\n",
    "# Recreate the ID (theater_id + date) from df_test_final\n",
    "df_test_final['ID'] = df_test_final['theater_id'].astype(str) + '_' + df_test_final['show_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Merge predictions back to the original submission format, preserving the exact ID order\n",
    "df_submission = pd.merge(\n",
    "    df_submission[['ID']], \n",
    "    df_test_final[['ID', 'audience_count']], \n",
    "    on='ID', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Final formatting\n",
    "df_submission['audience_count'].fillna(0, inplace=True)\n",
    "df_submission['audience_count'] = df_submission['audience_count'].round().astype(int)\n",
    "\n",
    "# Final check\n",
    "print(\"\\n--- Final Submission Dataframe Head (Improved Predictions) ---\")\n",
    "print(df_submission.head())\n",
    "print(f\"Submission file ready with {len(df_submission)} entries. This version should perform better due to advanced booking features!\")\n",
    "# df_submission.to_csv('lgbm_submission_v2_advanced.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fe7efab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 9: Top 10 Feature Importances ---\n",
      "theater_id         5179\n",
      "lag_7              5047\n",
      "rolling_mean_7     5037\n",
      "lag_14             4625\n",
      "day_of_week        2948\n",
      "booknow_tickets    1761\n",
      "latitude           1376\n",
      "theater_area       1295\n",
      "longitude          1116\n",
      "cinepos_tickets     866\n",
      "dtype: int32\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'model' and 'FEATURES' from the previous step are available\n",
    "\n",
    "# Get feature importance scores\n",
    "feature_imp = pd.Series(model.feature_importances_, index=FEATURES).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n--- Step 9: Top 10 Feature Importances ---\")\n",
    "print(feature_imp.head(10))\n",
    "\n",
    "# Plotting the top 10\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_imp.head(10).plot(kind='barh')\n",
    "plt.title('Top 10 LightGBM Feature Importance')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd6f9599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 10: Advanced Calendar Feature Engineering (Final Fix) ---\n",
      "Calendar and cyclical features created.\n",
      "Categorical features (including theater_id) robustly Label Encoded.\n",
      "\n",
      "Training set now has 24 total features.\n",
      "New Feature set: ['month', 'weekofyear', 'dayofmonth', 'dayofyear', 'dayofweek_num', 'is_weekend', 'dayofyear_sin', 'dayofyear_cos']...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n--- Step 10: Advanced Calendar Feature Engineering (Final Fix) ---\")\n",
    "\n",
    "# --- Combine Data for Consistent Feature Creation ---\n",
    "# Note: We temporary drop 'audience_count' from df_train_final for combination.\n",
    "df_combined = pd.concat([df_train_final.drop(columns=['audience_count', 'ID'], errors='ignore'), \n",
    "                         df_test_final.drop(columns=['audience_count', 'ID'], errors='ignore')], ignore_index=True)\n",
    "\n",
    "\n",
    "# --- 1. Basic Date Feature Extraction ---\n",
    "df_combined['year'] = df_combined['show_date'].dt.year\n",
    "df_combined['month'] = df_combined['show_date'].dt.month\n",
    "df_combined['weekofyear'] = df_combined['show_date'].dt.isocalendar().week.astype(int)\n",
    "df_combined['dayofmonth'] = df_combined['show_date'].dt.day\n",
    "df_combined['dayofyear'] = df_combined['show_date'].dt.dayofyear\n",
    "df_combined['dayofweek_num'] = df_combined['show_date'].dt.dayofweek # Monday=0, Sunday=6\n",
    "\n",
    "\n",
    "# --- 2. Weekend Indicator ---\n",
    "df_combined['is_weekend'] = df_combined['dayofweek_num'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "\n",
    "# --- 3. Cyclical Encoding (for Day of Year) ---\n",
    "DOY = df_combined['dayofyear']\n",
    "DAYS_IN_YEAR = 365.25 \n",
    "df_combined['dayofyear_sin'] = np.sin(2 * np.pi * DOY / DAYS_IN_YEAR)\n",
    "df_combined['dayofyear_cos'] = np.cos(2 * np.pi * DOY / DAYS_IN_YEAR)\n",
    "print(\"Calendar and cyclical features created.\")\n",
    "\n",
    "\n",
    "# --- 4. Re-split Data and Finalize Features ---\n",
    "df_train_new = df_combined.iloc[:len(df_train_final)].copy()\n",
    "df_test_new = df_combined.iloc[len(df_train_final):].copy()\n",
    "\n",
    "# Restore target variable to training set\n",
    "df_train_new['audience_count'] = df_train_final['audience_count']\n",
    "\n",
    "# --- FIX: Robust Label Encoding on Combined Values ---\n",
    "CAT_COLS = ['theater_id', 'day_of_week', 'theater_type', 'theater_area']\n",
    "for col in CAT_COLS:\n",
    "    le = LabelEncoder()\n",
    "    # Fit the encoder on the COMBINED unique values from both train and test\n",
    "    le.fit(pd.concat([df_train_new[col].astype(str), df_test_new[col].astype(str)]).unique())\n",
    "    \n",
    "    df_train_new[col] = le.transform(df_train_new[col].astype(str))\n",
    "    df_test_new[col] = le.transform(df_test_new[col].astype(str))\n",
    "print(\"Categorical features (including theater_id) robustly Label Encoded.\")\n",
    "\n",
    "\n",
    "# --- Define Final Feature Sets ---\n",
    "FEATURES = [col for col in df_train_new.columns if col not in ['show_date', 'audience_count']]\n",
    "TARGET = 'audience_count'\n",
    "\n",
    "X = df_train_new[FEATURES]\n",
    "y = df_train_new[TARGET]\n",
    "X_test = df_test_new[FEATURES]\n",
    "\n",
    "# FINAL CHECK: Ensure only features used in the model are present in X_test\n",
    "X_test = X_test[X.columns] \n",
    "\n",
    "print(f\"\\nTraining set now has {len(X.columns)} total features.\")\n",
    "print(f\"New Feature set: {X.columns.tolist()[-8:]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be9adc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 12.1: Model Training (LightGBM) with 24 Features ---\n",
      "LightGBM Model Training Complete.\n",
      "\n",
      "--- Step 12.2: Prediction and Submission ---\n",
      "\n",
      "--- Final Submission Dataframe Head (Version 3) ---\n",
      "                      ID  audience_count\n",
      "0  book_00001_2024-03-01               0\n",
      "1  book_00001_2024-03-02               0\n",
      "2  book_00001_2024-03-03               0\n",
      "3  book_00001_2024-03-04               0\n",
      "4  book_00001_2024-03-06               0\n",
      "Submission file ready with 38062 entries. This is your most feature-rich submission!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sajit\\AppData\\Local\\Temp\\ipykernel_23860\\3310466137.py:52: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_submission['audience_count'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "# --- 12.1: Model Training (LightGBM) ---\n",
    "print(\"\\n--- Step 12.1: Model Training (LightGBM) with 24 Features ---\")\n",
    "\n",
    "# The features and target are already defined as X, y, and X_test from the previous step.\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "model = lgb.LGBMRegressor(**params)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"LightGBM Model Training Complete.\")\n",
    "\n",
    "\n",
    "# --- 12.2: Prediction and Submission ---\n",
    "print(\"\\n--- Step 12.2: Prediction and Submission ---\")\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "predictions[predictions < 0] = 0\n",
    "\n",
    "# Assign predictions to the final test dataframe\n",
    "df_test_new['audience_count'] = predictions.round().astype(int)\n",
    "\n",
    "# Create Submission File\n",
    "df_submission = dfs['sample_submission'].copy()\n",
    "\n",
    "# Recreate the ID (theater_id + date) from df_test_new\n",
    "df_test_new['ID'] = df_test_new['theater_id'].astype(str) + '_' + df_test_new['show_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Merge predictions back to the original submission format, preserving the exact ID order\n",
    "df_submission = pd.merge(\n",
    "    df_submission[['ID']], \n",
    "    df_test_new[['ID', 'audience_count']], \n",
    "    on='ID', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Final formatting\n",
    "df_submission['audience_count'].fillna(0, inplace=True)\n",
    "df_submission['audience_count'] = df_submission['audience_count'].round().astype(int)\n",
    "\n",
    "# Final check\n",
    "print(\"\\n--- Final Submission Dataframe Head (Version 3) ---\")\n",
    "print(df_submission.head())\n",
    "print(f\"Submission file ready with {len(df_submission)} entries. This is your most feature-rich submission!\")\n",
    "# df_submission.to_csv('lgbm_submission_v3_final_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92897823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 13: Time-Series Cross-Validation (TSCV) ---\n",
      "Fold 1 completed. Validation RMSE: 22.6565\n",
      "Fold 2 completed. Validation RMSE: 24.4022\n",
      "Fold 3 completed. Validation RMSE: 20.6523\n",
      "\n",
      "--- Model Evaluation Complete ---\n",
      "Individual Fold RMSE Scores: [np.float64(22.656537269626753), np.float64(24.402227306832234), np.float64(20.652346283726057)]\n",
      "Average Cross-Validation RMSE (Your Estimated Score): 22.5704\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "print(\"\\n--- Step 13: Time-Series Cross-Validation (TSCV) ---\")\n",
    "\n",
    "# Define TSCV: 3 splits are usually sufficient for a quick check\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "oof_predictions = np.zeros(X.shape[0]) # Out-Of-Fold predictions array\n",
    "rmse_scores = []\n",
    "fold_count = 1\n",
    "\n",
    "# Base Model Parameters\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Loop through each time series split\n",
    "for train_index, val_index in tscv.split(X):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # Train the model\n",
    "    model_tscv = lgb.LGBMRegressor(**params)\n",
    "    model_tscv.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric='rmse',\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    "    )\n",
    "\n",
    "    # Predict on the validation set and calculate RMSE\n",
    "    val_preds = model_tscv.predict(X_val)\n",
    "    val_preds[val_preds < 0] = 0\n",
    "    oof_predictions[val_index] = val_preds\n",
    "\n",
    "    fold_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "    rmse_scores.append(fold_rmse)\n",
    "\n",
    "    print(f\"Fold {fold_count} completed. Validation RMSE: {fold_rmse:.4f}\")\n",
    "    fold_count += 1\n",
    "\n",
    "# Calculate the final OOF (Out-Of-Fold) score\n",
    "average_rmse = np.mean(rmse_scores)\n",
    "\n",
    "print(f\"\\n--- Model Evaluation Complete ---\")\n",
    "print(f\"Individual Fold RMSE Scores: {rmse_scores}\")\n",
    "print(f\"Average Cross-Validation RMSE (Your Estimated Score): {average_rmse:.4f}\")\n",
    "\n",
    "# Final step: Retrain on all data using optimal n_estimators found via early stopping\n",
    "# We won't find the exact best n_estimators here, but the average RMSE is your key metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "116a4390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 14: Hyperparameter Tuning with GridSearchCV ---\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=20, n_estimators=500, num_leaves=31; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=20, n_estimators=500, num_leaves=31; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=20, n_estimators=500, num_leaves=31; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=20, n_estimators=500, num_leaves=50; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=20, n_estimators=500, num_leaves=50; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=20, n_estimators=500, num_leaves=50; total time=   1.7s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=20, n_estimators=1000, num_leaves=31; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=20, n_estimators=1000, num_leaves=31; total time=   1.9s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=20, n_estimators=1000, num_leaves=31; total time=   2.1s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=20, n_estimators=1000, num_leaves=50; total time=   1.7s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=20, n_estimators=1000, num_leaves=50; total time=   2.1s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=20, n_estimators=1000, num_leaves=50; total time=   2.5s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=50, n_estimators=500, num_leaves=31; total time=   0.7s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=50, n_estimators=500, num_leaves=31; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=50, n_estimators=500, num_leaves=31; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=50, n_estimators=500, num_leaves=50; total time=   0.9s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=50, n_estimators=500, num_leaves=50; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=50, n_estimators=500, num_leaves=50; total time=   1.7s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=50, n_estimators=1000, num_leaves=31; total time=   1.9s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=50, n_estimators=1000, num_leaves=31; total time=   2.3s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=50, n_estimators=1000, num_leaves=31; total time=   2.4s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=50, n_estimators=1000, num_leaves=50; total time=   2.0s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=50, n_estimators=1000, num_leaves=50; total time=   2.6s\n",
      "[CV] END learning_rate=0.03, max_depth=7, min_child_samples=50, n_estimators=1000, num_leaves=50; total time=   3.2s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=31; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=31; total time=   1.3s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=31; total time=   1.5s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=50; total time=   1.2s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=50; total time=   1.7s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=50; total time=   1.9s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=20, n_estimators=1000, num_leaves=31; total time=   2.1s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=20, n_estimators=1000, num_leaves=31; total time=   2.3s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=20, n_estimators=1000, num_leaves=31; total time=   3.2s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=20, n_estimators=1000, num_leaves=50; total time=   2.7s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=20, n_estimators=1000, num_leaves=50; total time=   3.0s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=20, n_estimators=1000, num_leaves=50; total time=   3.2s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=50, n_estimators=500, num_leaves=31; total time=   0.8s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=50, n_estimators=500, num_leaves=31; total time=   1.1s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=50, n_estimators=500, num_leaves=31; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=50, n_estimators=500, num_leaves=50; total time=   1.0s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=50, n_estimators=500, num_leaves=50; total time=   1.4s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=50, n_estimators=500, num_leaves=50; total time=   1.7s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=50, n_estimators=1000, num_leaves=31; total time=   1.6s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=50, n_estimators=1000, num_leaves=31; total time=   1.9s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=50, n_estimators=1000, num_leaves=31; total time=   2.5s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=50, n_estimators=1000, num_leaves=50; total time=   2.3s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=50, n_estimators=1000, num_leaves=50; total time=   2.6s\n",
      "[CV] END learning_rate=0.03, max_depth=10, min_child_samples=50, n_estimators=1000, num_leaves=50; total time=   3.0s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=20, n_estimators=500, num_leaves=31; total time=   0.7s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=20, n_estimators=500, num_leaves=31; total time=   0.9s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=20, n_estimators=500, num_leaves=31; total time=   1.1s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=20, n_estimators=500, num_leaves=50; total time=   0.9s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=20, n_estimators=500, num_leaves=50; total time=   1.1s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=20, n_estimators=500, num_leaves=50; total time=   1.3s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=20, n_estimators=1000, num_leaves=31; total time=   1.4s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=20, n_estimators=1000, num_leaves=31; total time=   1.8s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=20, n_estimators=1000, num_leaves=31; total time=   2.1s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=20, n_estimators=1000, num_leaves=50; total time=   1.8s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=20, n_estimators=1000, num_leaves=50; total time=   2.2s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=20, n_estimators=1000, num_leaves=50; total time=   2.6s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=50, n_estimators=500, num_leaves=31; total time=   0.7s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=50, n_estimators=500, num_leaves=31; total time=   0.9s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=50, n_estimators=500, num_leaves=31; total time=   1.2s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=50, n_estimators=500, num_leaves=50; total time=   0.9s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=50, n_estimators=500, num_leaves=50; total time=   1.1s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=50, n_estimators=500, num_leaves=50; total time=   1.3s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=50, n_estimators=1000, num_leaves=31; total time=   1.4s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=50, n_estimators=1000, num_leaves=31; total time=   1.8s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=50, n_estimators=1000, num_leaves=31; total time=   2.2s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=50, n_estimators=1000, num_leaves=50; total time=   1.6s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=50, n_estimators=1000, num_leaves=50; total time=   2.8s\n",
      "[CV] END learning_rate=0.05, max_depth=7, min_child_samples=50, n_estimators=1000, num_leaves=50; total time=   3.0s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=31; total time=   0.7s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=31; total time=   1.0s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=31; total time=   1.2s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=50; total time=   1.1s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=50; total time=   1.3s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=20, n_estimators=500, num_leaves=50; total time=   1.6s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=20, n_estimators=1000, num_leaves=31; total time=   1.7s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=20, n_estimators=1000, num_leaves=31; total time=   2.4s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=20, n_estimators=1000, num_leaves=31; total time=   2.6s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=20, n_estimators=1000, num_leaves=50; total time=   2.7s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=20, n_estimators=1000, num_leaves=50; total time=   2.8s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=20, n_estimators=1000, num_leaves=50; total time=   3.7s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=50, n_estimators=500, num_leaves=31; total time=   1.6s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=50, n_estimators=500, num_leaves=31; total time=   1.3s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=50, n_estimators=500, num_leaves=31; total time=   1.5s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=50, n_estimators=500, num_leaves=50; total time=   1.2s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=50, n_estimators=500, num_leaves=50; total time=   1.8s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=50, n_estimators=500, num_leaves=50; total time=   1.8s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=50, n_estimators=1000, num_leaves=31; total time=   1.8s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=50, n_estimators=1000, num_leaves=31; total time=   2.2s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=50, n_estimators=1000, num_leaves=31; total time=   2.9s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=50, n_estimators=1000, num_leaves=50; total time=   3.0s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=50, n_estimators=1000, num_leaves=50; total time=   3.4s\n",
      "[CV] END learning_rate=0.05, max_depth=10, min_child_samples=50, n_estimators=1000, num_leaves=50; total time=   9.1s\n",
      "\n",
      "--- GridSearchCV Complete ---\n",
      "Best Parameters found: {'learning_rate': 0.03, 'max_depth': 7, 'min_child_samples': 50, 'n_estimators': 500, 'num_leaves': 31}\n",
      "Best CV RMSE: 4.7577\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"\\n--- Step 14: Hyperparameter Tuning with GridSearchCV ---\")\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [500, 1000],\n",
    "    'learning_rate': [0.03, 0.05],\n",
    "    'num_leaves': [31, 50],\n",
    "    'max_depth': [7, 10],\n",
    "    'min_child_samples': [20, 50]\n",
    "}\n",
    "\n",
    "# Use the same TimeSeriesSplit for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# Initialize the base model\n",
    "model_grid = lgb.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    metric='rmse',\n",
    "    bagging_fraction=0.8,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_freq=1,\n",
    "    verbose=-1,\n",
    "    n_jobs=-1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "gscv = GridSearchCV(\n",
    "    estimator=model_grid,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=tscv,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the grid search to the data\n",
    "gscv.fit(X, y)\n",
    "\n",
    "print(\"\\n--- GridSearchCV Complete ---\")\n",
    "print(f\"Best Parameters found: {gscv.best_params_}\")\n",
    "print(f\"Best CV RMSE: {np.sqrt(-gscv.best_score_):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16fb88d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Model Training with Optimal Parameters (Fixed) ---\n",
      "Final Model Training Complete with Optimal Parameters.\n",
      "\n",
      "--- Final Prediction and Submission ---\n",
      "\n",
      "--- Final Submission Dataframe Head (Optimized Version) ---\n",
      "Best CV RMSE found: 22.7164\n",
      "Best Parameters: {'learning_rate': 0.03, 'max_depth': 10, 'min_child_samples': 20, 'n_estimators': 500, 'num_leaves': 31}\n",
      "                      ID  audience_count\n",
      "0  book_00001_2024-03-01               0\n",
      "1  book_00001_2024-03-02               0\n",
      "2  book_00001_2024-03-03               0\n",
      "3  book_00001_2024-03-04               0\n",
      "4  book_00001_2024-03-06               0\n",
      "Optimization complete. Your final submission file is ready! (Version 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sajit\\AppData\\Local\\Temp\\ipykernel_23860\\1148477664.py:56: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_submission['audience_count'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd # Import pandas for df_submission creation\n",
    "\n",
    "# Assume X, y, X_test, dfs, df_test_new, gscv, best_params are available from previous steps\n",
    "\n",
    "print(\"\\n--- Final Model Training with Optimal Parameters (Fixed) ---\")\n",
    "\n",
    "# 1. Get the best parameters from the GridSearch result\n",
    "best_params = gscv.best_params_\n",
    "\n",
    "# 2. Define the Base Model parameters (excluding any that were in the grid to avoid conflict)\n",
    "base_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 42,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# 3. Create the final parameter dictionary by merging best_params over base_params\n",
    "final_params = base_params.copy()\n",
    "final_params.update(best_params)\n",
    "\n",
    "# 4. Train the final model using the combined optimal parameters\n",
    "final_model = lgb.LGBMRegressor(**final_params)\n",
    "\n",
    "# Note: We skip the early stopping here as we already determined the best fixed n_estimators=500/1000\n",
    "final_model.fit(X, y)\n",
    "\n",
    "print(\"Final Model Training Complete with Optimal Parameters.\")\n",
    "\n",
    "\n",
    "# --- Final Prediction and Submission (Best Version) ---\n",
    "print(\"\\n--- Final Prediction and Submission ---\")\n",
    "\n",
    "predictions_final = final_model.predict(X_test)\n",
    "predictions_final[predictions_final < 0] = 0\n",
    "\n",
    "# Assign predictions to the final test dataframe\n",
    "df_test_new['audience_count'] = predictions_final.round().astype(int)\n",
    "\n",
    "# Create Submission File\n",
    "df_submission = dfs['sample_submission'].copy()\n",
    "df_test_new['ID'] = df_test_new['theater_id'].astype(str) + '_' + df_test_new['show_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "df_submission = pd.merge(\n",
    "    df_submission[['ID']], \n",
    "    df_test_new[['ID', 'audience_count']], \n",
    "    on='ID', \n",
    "    how='left'\n",
    ")\n",
    "df_submission['audience_count'].fillna(0, inplace=True)\n",
    "df_submission['audience_count'] = df_submission['audience_count'].round().astype(int)\n",
    "\n",
    "\n",
    "print(\"\\n--- Final Submission Dataframe Head (Optimized Version) ---\")\n",
    "print(f\"Best CV RMSE found: {np.sqrt(-gscv.best_score_):.4f}\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(df_submission.head())\n",
    "print(\"Optimization complete. Your final submission file is ready! (Version 4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45668c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 15: Calculating SMAPE (Percentage Accuracy) via TSCV ---\n",
      "Fold 1 completed. Validation SMAPE: 45.9261%\n",
      "Fold 2 completed. Validation SMAPE: 41.9733%\n",
      "Fold 3 completed. Validation SMAPE: 42.3420%\n",
      "\n",
      "--- Model Evaluation Complete ---\n",
      "Average Cross-Validation SMAPE (Your Estimated Percentage Accuracy): 43.4138%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n--- Step 15: Calculating SMAPE (Percentage Accuracy) via TSCV ---\")\n",
    "\n",
    "# Best Parameters found in the previous step\n",
    "best_params = {'learning_rate': 0.03, 'max_depth': 10, 'min_child_samples': 20, 'n_estimators': 500, 'num_leaves': 31}\n",
    "base_params = {\n",
    "    'objective': 'regression', 'metric': 'rmse', 'n_jobs': -1, 'random_state': 42,\n",
    "    'bagging_fraction': 0.8, 'feature_fraction': 0.8, 'bagging_freq': 1, 'verbose': -1\n",
    "}\n",
    "final_params = base_params.copy()\n",
    "final_params.update(best_params)\n",
    "\n",
    "# Function to calculate SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    # Handle division by zero (where A_t and F_t are both 0)\n",
    "    return np.mean(numerator / np.where(denominator == 0, 1, denominator)) * 100\n",
    "\n",
    "# Define TSCV\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "oof_predictions = np.zeros(X.shape[0]) # Out-Of-Fold predictions array\n",
    "smape_scores = []\n",
    "fold_count = 1\n",
    "\n",
    "# Loop through each time series split\n",
    "for train_index, val_index in tscv.split(X):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # Train the model with optimal parameters\n",
    "    model_tscv = lgb.LGBMRegressor(**final_params)\n",
    "    model_tscv.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='rmse', callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)])\n",
    "\n",
    "    # Predict on the validation set\n",
    "    val_preds = model_tscv.predict(X_val)\n",
    "    val_preds[val_preds < 0] = 0\n",
    "    oof_predictions[val_index] = val_preds\n",
    "\n",
    "    # Calculate SMAPE for the fold\n",
    "    fold_smape = smape(y_val.values, val_preds)\n",
    "    smape_scores.append(fold_smape)\n",
    "\n",
    "    print(f\"Fold {fold_count} completed. Validation SMAPE: {fold_smape:.4f}%\")\n",
    "    fold_count += 1\n",
    "\n",
    "average_smape = np.mean(smape_scores)\n",
    "print(f\"\\n--- Model Evaluation Complete ---\")\n",
    "print(f\"Average Cross-Validation SMAPE (Your Estimated Percentage Accuracy): {average_smape:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5519e316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 16: Advanced Feature Engineering - Target Encoding ---\n",
      "Target Encoded features ('avg_audience_theater', 'avg_audience_theater_day') created and merged.\n",
      "Train shape after merge: (214046, 37)\n",
      "Test shape after merge: (38062, 38)\n",
      "\n",
      "--- Null check for new features in test set ---\n",
      "avg_audience_theater        0\n",
      "avg_audience_theater_day    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sajit\\AppData\\Local\\Temp\\ipykernel_23860\\215476166.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_test_new['avg_audience_theater'].fillna(overall_mean_audience, inplace=True)\n",
      "C:\\Users\\sajit\\AppData\\Local\\Temp\\ipykernel_23860\\215476166.py:29: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_test_new['avg_audience_theater_day'].fillna(df_test_new['avg_audience_theater'], inplace=True) # Use theater avg as fallback\n",
      "C:\\Users\\sajit\\AppData\\Local\\Temp\\ipykernel_23860\\215476166.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_train_new['avg_audience_theater_day'].fillna(df_train_new['avg_audience_theater'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"\\n--- Step 16: Advanced Feature Engineering - Target Encoding ---\")\n",
    "\n",
    "# --- 1. Average Audience per Theater ---\n",
    "# Calculate the mean audience for each theater_id from the training data\n",
    "theater_avg = df_train_final.groupby('theater_id')['audience_count'].mean().reset_index()\n",
    "theater_avg.rename(columns={'audience_count': 'avg_audience_theater'}, inplace=True)\n",
    "\n",
    "# --- 2. Average Audience per Theater and Day-of-Week ---\n",
    "# Calculate the mean audience for each theater_id and day_of_week combination\n",
    "theater_day_avg = df_train_final.groupby(['theater_id', 'day_of_week'])['audience_count'].mean().reset_index()\n",
    "theater_day_avg.rename(columns={'audience_count': 'avg_audience_theater_day'}, inplace=True)\n",
    "\n",
    "\n",
    "# --- 3. Merge New Features into Training and Test Sets ---\n",
    "# Use the new dataframes (df_train_new, df_test_new) from the calendar feature step\n",
    "df_train_new = pd.merge(df_train_new, theater_avg, on='theater_id', how='left')\n",
    "df_train_new = pd.merge(df_train_new, theater_day_avg, on=['theater_id', 'day_of_week'], how='left')\n",
    "\n",
    "df_test_new = pd.merge(df_test_new, theater_avg, on='theater_id', how='left')\n",
    "df_test_new = pd.merge(df_test_new, theater_day_avg, on=['theater_id', 'day_of_week'], how='left')\n",
    "\n",
    "\n",
    "# --- 4. Impute Missing Values in Test Set ---\n",
    "# For any theater/day in the test set not seen in training, fill with the overall average audience\n",
    "overall_mean_audience = df_train_final['audience_count'].mean()\n",
    "df_test_new['avg_audience_theater'].fillna(overall_mean_audience, inplace=True)\n",
    "df_test_new['avg_audience_theater_day'].fillna(df_test_new['avg_audience_theater'], inplace=True) # Use theater avg as fallback\n",
    "df_train_new['avg_audience_theater_day'].fillna(df_train_new['avg_audience_theater'], inplace=True)\n",
    "\n",
    "\n",
    "print(\"Target Encoded features ('avg_audience_theater', 'avg_audience_theater_day') created and merged.\")\n",
    "print(f\"Train shape after merge: {df_train_new.shape}\")\n",
    "print(f\"Test shape after merge: {df_test_new.shape}\")\n",
    "print(\"\\n--- Null check for new features in test set ---\")\n",
    "print(df_test_new[['avg_audience_theater', 'avg_audience_theater_day']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bba57d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 17: Advanced Feature Engineering - Lagged Advance Bookings (FIXED) ---\n",
      "Lagged Advance Booking features created and merged.\n",
      "Train shape after merge: (214046, 41)\n",
      "Test shape after merge: (38062, 42)\n",
      "\n",
      "--- Head of test set with new lagged features ---\n",
      "   theater_id  show_date  adv_1d_lag7  adv_total_lag7\n",
      "0           0 2024-03-01          0.0             0.0\n",
      "1           0 2024-03-02          0.0             0.0\n",
      "2           0 2024-03-03          0.0             0.0\n",
      "3           0 2024-03-04          0.0             0.0\n",
      "4           0 2024-03-06          0.0             0.0\n",
      "Lagged Advance Booking features created and merged.\n",
      "Train shape after merge: (214046, 41)\n",
      "Test shape after merge: (38062, 42)\n",
      "\n",
      "--- Head of test set with new lagged features ---\n",
      "   theater_id  show_date  adv_1d_lag7  adv_total_lag7\n",
      "0           0 2024-03-01          0.0             0.0\n",
      "1           0 2024-03-02          0.0             0.0\n",
      "2           0 2024-03-03          0.0             0.0\n",
      "3           0 2024-03-04          0.0             0.0\n",
      "4           0 2024-03-06          0.0             0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sajit\\AppData\\Local\\Temp\\ipykernel_23860\\2625275131.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_train_new[col].fillna(0, inplace=True)\n",
      "C:\\Users\\sajit\\AppData\\Local\\Temp\\ipykernel_23860\\2625275131.py:46: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_test_new[col].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"\\n--- Step 17: Advanced Feature Engineering - Lagged Advance Bookings (FIXED) ---\")\n",
    "\n",
    "# We need the pivoted advance booking data from Step 7\n",
    "# Assuming df_adv_pivot is available\n",
    "\n",
    "# --- FIX: Ensure consistent data types for merging ---\n",
    "# df_adv_pivot has theater_id as string (object), so we must match it.\n",
    "df_train_new['theater_id'] = df_train_new['theater_id'].astype(str)\n",
    "df_test_new['theater_id'] = df_test_new['theater_id'].astype(str)\n",
    "df_adv_pivot['theater_id'] = df_adv_pivot['theater_id'].astype(str)\n",
    "\n",
    "\n",
    "# Create a full date range for each theater to correctly calculate lags\n",
    "df_full_range = pd.concat([\n",
    "    df_train_new[['theater_id', 'show_date']],\n",
    "    df_test_new[['theater_id', 'show_date']]\n",
    "], ignore_index=True).drop_duplicates()\n",
    "\n",
    "# Merge the advance booking data onto this full range\n",
    "df_adv_full = pd.merge(\n",
    "    df_full_range,\n",
    "    df_adv_pivot,\n",
    "    on=['theater_id', 'show_date'],\n",
    "    how='left'\n",
    ").fillna(0)\n",
    "\n",
    "# Sort for time-series operations\n",
    "df_adv_full.sort_values(['theater_id', 'show_date'], inplace=True)\n",
    "\n",
    "# Create 7-day lag features for each advance booking bin\n",
    "for col in ['adv_1d', 'adv_3d', 'adv_7d', 'adv_total']:\n",
    "    df_adv_full[f'{col}_lag7'] = df_adv_full.groupby('theater_id')[col].shift(7)\n",
    "\n",
    "# Select only the lagged columns and keys for merging\n",
    "df_adv_lagged = df_adv_full[['theater_id', 'show_date', 'adv_1d_lag7', 'adv_3d_lag7', 'adv_7d_lag7', 'adv_total_lag7']]\n",
    "\n",
    "# Merge lagged features into the main dataframes\n",
    "df_train_new = pd.merge(df_train_new, df_adv_lagged, on=['theater_id', 'show_date'], how='left')\n",
    "df_test_new = pd.merge(df_test_new, df_adv_lagged, on=['theater_id', 'show_date'], how='left')\n",
    "\n",
    "# Fill NaNs with 0 (for the start of the time series where no lag is available)\n",
    "for col in ['adv_1d_lag7', 'adv_3d_lag7', 'adv_7d_lag7', 'adv_total_lag7']:\n",
    "    df_train_new[col].fillna(0, inplace=True)\n",
    "    df_test_new[col].fillna(0, inplace=True)\n",
    "\n",
    "# --- FIX: Convert theater_id back to integer for the model ---\n",
    "le = LabelEncoder()\n",
    "le.fit(pd.concat([df_train_new['theater_id'], df_test_new['theater_id']]).unique())\n",
    "df_train_new['theater_id'] = le.transform(df_train_new['theater_id'])\n",
    "df_test_new['theater_id'] = le.transform(df_test_new['theater_id'])\n",
    "\n",
    "\n",
    "print(\"Lagged Advance Booking features created and merged.\")\n",
    "print(f\"Train shape after merge: {df_train_new.shape}\")\n",
    "print(f\"Test shape after merge: {df_test_new.shape}\")\n",
    "print(\"\\n--- Head of test set with new lagged features ---\")\n",
    "print(df_test_new[['theater_id', 'show_date', 'adv_1d_lag7', 'adv_total_lag7']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4072954e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 18: Final Model Training with All Features ---\n",
      "Training final model with 39 features.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: theater_id_str: object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# --- 2. Train Final Model with Best Parameters ---\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Using best_params from the GridSearchCV step\u001b[39;00m\n\u001b[0;32m     22\u001b[0m final_model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgscv\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[1;32m---> 23\u001b[0m \u001b[43mfinal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal model training complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# --- 3. Final Prediction and Submission ---\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\Kaggle Challenge\\theatre\\lib\\site-packages\\lightgbm\\sklearn.py:1398\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m   1382\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1383\u001b[0m     X: _LGBM_ScikitMatrixLike,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1395\u001b[0m     init_model: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path, Booster, LGBMModel]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1396\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLGBMRegressor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1397\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1398\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1400\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1402\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1411\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\Projects\\Kaggle Challenge\\theatre\\lib\\site-packages\\lightgbm\\sklearn.py:1049\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1046\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1047\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m-> 1049\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mnum_feature()\n",
      "File \u001b[1;32md:\\Projects\\Kaggle Challenge\\theatre\\lib\\site-packages\\lightgbm\\engine.py:297\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 297\u001b[0m     booster \u001b[38;5;241m=\u001b[39m \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m    299\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[1;32md:\\Projects\\Kaggle Challenge\\theatre\\lib\\site-packages\\lightgbm\\basic.py:3656\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[1;34m(self, params, train_set, model_file, model_str)\u001b[0m\n\u001b[0;32m   3649\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_network(\n\u001b[0;32m   3650\u001b[0m         machines\u001b[38;5;241m=\u001b[39mmachines,\n\u001b[0;32m   3651\u001b[0m         local_listen_port\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_listen_port\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   3652\u001b[0m         listen_time_out\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_out\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m120\u001b[39m),\n\u001b[0;32m   3653\u001b[0m         num_machines\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_machines\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   3654\u001b[0m     )\n\u001b[0;32m   3655\u001b[0m \u001b[38;5;66;03m# construct booster object\u001b[39;00m\n\u001b[1;32m-> 3656\u001b[0m \u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3657\u001b[0m \u001b[38;5;66;03m# copy the parameters from train_set\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(train_set\u001b[38;5;241m.\u001b[39mget_params())\n",
      "File \u001b[1;32md:\\Projects\\Kaggle Challenge\\theatre\\lib\\site-packages\\lightgbm\\basic.py:2590\u001b[0m, in \u001b[0;36mDataset.construct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2585\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_init_score_by_predictor(\n\u001b[0;32m   2586\u001b[0m                 predictor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictor, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, used_indices\u001b[38;5;241m=\u001b[39mused_indices\n\u001b[0;32m   2587\u001b[0m             )\n\u001b[0;32m   2588\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2589\u001b[0m     \u001b[38;5;66;03m# create train\u001b[39;00m\n\u001b[1;32m-> 2590\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2596\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfree_raw_data:\n\u001b[0;32m   2604\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\Kaggle Challenge\\theatre\\lib\\site-packages\\lightgbm\\basic.py:2123\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, feature_name, categorical_feature, params, position)\u001b[0m\n\u001b[0;32m   2121\u001b[0m     categorical_feature \u001b[38;5;241m=\u001b[39m reference\u001b[38;5;241m.\u001b[39mcategorical_feature\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd_DataFrame):\n\u001b[1;32m-> 2123\u001b[0m     data, feature_name, categorical_feature, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpandas_categorical \u001b[38;5;241m=\u001b[39m \u001b[43m_data_from_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpandas_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpandas_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_pyarrow_table(data) \u001b[38;5;129;01mand\u001b[39;00m feature_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2130\u001b[0m     feature_name \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumn_names\n",
      "File \u001b[1;32md:\\Projects\\Kaggle Challenge\\theatre\\lib\\site-packages\\lightgbm\\basic.py:868\u001b[0m, in \u001b[0;36m_data_from_pandas\u001b[1;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[0;32m    864\u001b[0m df_dtypes\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    865\u001b[0m target_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mresult_type(\u001b[38;5;241m*\u001b[39mdf_dtypes)\n\u001b[0;32m    867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 868\u001b[0m     \u001b[43m_pandas_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    869\u001b[0m     feature_name,\n\u001b[0;32m    870\u001b[0m     categorical_feature,\n\u001b[0;32m    871\u001b[0m     pandas_categorical,\n\u001b[0;32m    872\u001b[0m )\n",
      "File \u001b[1;32md:\\Projects\\Kaggle Challenge\\theatre\\lib\\site-packages\\lightgbm\\basic.py:814\u001b[0m, in \u001b[0;36m_pandas_to_numpy\u001b[1;34m(data, target_dtype)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_pandas_to_numpy\u001b[39m(\n\u001b[0;32m    811\u001b[0m     data: pd_DataFrame,\n\u001b[0;32m    812\u001b[0m     target_dtype: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.typing.DTypeLike\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    813\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m--> 814\u001b[0m     \u001b[43m_check_for_bad_pandas_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;66;03m# most common case (no nullable dtypes)\u001b[39;00m\n\u001b[0;32m    817\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\u001b[38;5;241m.\u001b[39mto_numpy(dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Projects\\Kaggle Challenge\\theatre\\lib\\site-packages\\lightgbm\\basic.py:805\u001b[0m, in \u001b[0;36m_check_for_bad_pandas_dtypes\u001b[1;34m(pandas_dtypes_series)\u001b[0m\n\u001b[0;32m    799\u001b[0m bad_pandas_dtypes \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpandas_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m column_name, pandas_dtype \u001b[38;5;129;01min\u001b[39;00m pandas_dtypes_series\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_allowed_numpy_dtype(pandas_dtype\u001b[38;5;241m.\u001b[39mtype)\n\u001b[0;32m    803\u001b[0m ]\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bad_pandas_dtypes:\n\u001b[1;32m--> 805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas dtypes must be int, float or bool.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFields with bad pandas dtypes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(bad_pandas_dtypes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    807\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: theater_id_str: object"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n--- Step 18: Final Model Training with All Features ---\")\n",
    "\n",
    "# --- 1. Define Final Feature Set ---\n",
    "FEATURES = [col for col in df_train_new.columns if col not in ['show_date', 'audience_count', 'ID']]\n",
    "TARGET = 'audience_count'\n",
    "\n",
    "X = df_train_new[FEATURES]\n",
    "y = df_train_new[TARGET]\n",
    "X_test = df_test_new[FEATURES]\n",
    "\n",
    "# Ensure columns are in the same order\n",
    "X_test = X_test[X.columns]\n",
    "\n",
    "print(f\"Training final model with {len(FEATURES)} features.\")\n",
    "\n",
    "# --- 2. Train Final Model with Best Parameters ---\n",
    "# Using best_params from the GridSearchCV step\n",
    "final_model = lgb.LGBMRegressor(**gscv.best_params_)\n",
    "final_model.fit(X, y)\n",
    "\n",
    "print(\"Final model training complete.\")\n",
    "\n",
    "# --- 3. Final Prediction and Submission ---\n",
    "print(\"\\n--- Generating Final Production-Grade Submission ---\")\n",
    "\n",
    "predictions_final = final_model.predict(X_test)\n",
    "predictions_final[predictions_final < 0] = 0\n",
    "\n",
    "df_test_new['audience_count'] = predictions_final.round().astype(int)\n",
    "\n",
    "# Create Submission File\n",
    "df_submission = dfs['sample_submission'].copy()\n",
    "df_test_new['ID'] = df_test_new['theater_id'].astype(str) + '_' + df_test_new['show_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "df_submission = pd.merge(\n",
    "    df_submission[['ID']],\n",
    "    df_test_new[['ID', 'audience_count']],\n",
    "    on='ID',\n",
    "    how='left'\n",
    ")\n",
    "df_submission['audience_count'].fillna(0, inplace=True)\n",
    "df_submission['audience_count'] = df_submission['audience_count'].round().astype(int)\n",
    "\n",
    "print(\"\\n--- Final Submission Dataframe Head (Production Version) ---\")\n",
    "print(df_submission.head())\n",
    "print(f\"Submission file ready with {len(df_submission)} entries.\")\n",
    "# df_submission.to_csv('lgbm_submission_production.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "24e6c752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 19: Performance Evaluation with SMAPE ---\n",
      "Running 3-fold time-series cross-validation to estimate SMAPE...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: theater_id_str: object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Train model using best parameters\u001b[39;00m\n\u001b[0;32m     29\u001b[0m model_eval \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgscv\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[1;32m---> 30\u001b[0m \u001b[43mmodel_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Predict on validation set\u001b[39;00m\n\u001b[0;32m     33\u001b[0m val_preds \u001b[38;5;241m=\u001b[39m model_eval\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[1;32md:\\Projects\\Kaggle Challenge\\theatre\\lib\\site-packages\\lightgbm\\sklearn.py:1398\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m   1382\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1383\u001b[0m     X: _LGBM_ScikitMatrixLike,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1395\u001b[0m     init_model: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path, Booster, LGBMModel]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1396\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLGBMRegressor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1397\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1398\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1400\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1402\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1411\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\Projects\\Kaggle Challenge\\theatre\\lib\\site-packages\\lightgbm\\sklearn.py:1049\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1046\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1047\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m-> 1049\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mnum_feature()\n",
      "File \u001b[1;32md:\\Projects\\Kaggle Challenge\\theatre\\lib\\site-packages\\lightgbm\\engine.py:297\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 297\u001b[0m     booster \u001b[38;5;241m=\u001b[39m \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m    299\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[1;32md:\\Projects\\Kaggle Challenge\\theatre\\lib\\site-packages\\lightgbm\\basic.py:3656\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[1;34m(self, params, train_set, model_file, model_str)\u001b[0m\n\u001b[0;32m   3649\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_network(\n\u001b[0;32m   3650\u001b[0m         machines\u001b[38;5;241m=\u001b[39mmachines,\n\u001b[0;32m   3651\u001b[0m         local_listen_port\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_listen_port\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   3652\u001b[0m         listen_time_out\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_out\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m120\u001b[39m),\n\u001b[0;32m   3653\u001b[0m         num_machines\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_machines\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   3654\u001b[0m     )\n\u001b[0;32m   3655\u001b[0m \u001b[38;5;66;03m# construct booster object\u001b[39;00m\n\u001b[1;32m-> 3656\u001b[0m \u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3657\u001b[0m \u001b[38;5;66;03m# copy the parameters from train_set\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(train_set\u001b[38;5;241m.\u001b[39mget_params())\n",
      "File \u001b[1;32md:\\Projects\\Kaggle Challenge\\theatre\\lib\\site-packages\\lightgbm\\basic.py:2590\u001b[0m, in \u001b[0;36mDataset.construct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2585\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_init_score_by_predictor(\n\u001b[0;32m   2586\u001b[0m                 predictor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictor, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, used_indices\u001b[38;5;241m=\u001b[39mused_indices\n\u001b[0;32m   2587\u001b[0m             )\n\u001b[0;32m   2588\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2589\u001b[0m     \u001b[38;5;66;03m# create train\u001b[39;00m\n\u001b[1;32m-> 2590\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2596\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfree_raw_data:\n\u001b[0;32m   2604\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\Kaggle Challenge\\theatre\\lib\\site-packages\\lightgbm\\basic.py:2123\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, feature_name, categorical_feature, params, position)\u001b[0m\n\u001b[0;32m   2121\u001b[0m     categorical_feature \u001b[38;5;241m=\u001b[39m reference\u001b[38;5;241m.\u001b[39mcategorical_feature\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd_DataFrame):\n\u001b[1;32m-> 2123\u001b[0m     data, feature_name, categorical_feature, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpandas_categorical \u001b[38;5;241m=\u001b[39m \u001b[43m_data_from_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpandas_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpandas_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_pyarrow_table(data) \u001b[38;5;129;01mand\u001b[39;00m feature_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2130\u001b[0m     feature_name \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumn_names\n",
      "File \u001b[1;32md:\\Projects\\Kaggle Challenge\\theatre\\lib\\site-packages\\lightgbm\\basic.py:868\u001b[0m, in \u001b[0;36m_data_from_pandas\u001b[1;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[0;32m    864\u001b[0m df_dtypes\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    865\u001b[0m target_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mresult_type(\u001b[38;5;241m*\u001b[39mdf_dtypes)\n\u001b[0;32m    867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 868\u001b[0m     \u001b[43m_pandas_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    869\u001b[0m     feature_name,\n\u001b[0;32m    870\u001b[0m     categorical_feature,\n\u001b[0;32m    871\u001b[0m     pandas_categorical,\n\u001b[0;32m    872\u001b[0m )\n",
      "File \u001b[1;32md:\\Projects\\Kaggle Challenge\\theatre\\lib\\site-packages\\lightgbm\\basic.py:814\u001b[0m, in \u001b[0;36m_pandas_to_numpy\u001b[1;34m(data, target_dtype)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_pandas_to_numpy\u001b[39m(\n\u001b[0;32m    811\u001b[0m     data: pd_DataFrame,\n\u001b[0;32m    812\u001b[0m     target_dtype: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.typing.DTypeLike\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    813\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m--> 814\u001b[0m     \u001b[43m_check_for_bad_pandas_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;66;03m# most common case (no nullable dtypes)\u001b[39;00m\n\u001b[0;32m    817\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\u001b[38;5;241m.\u001b[39mto_numpy(dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Projects\\Kaggle Challenge\\theatre\\lib\\site-packages\\lightgbm\\basic.py:805\u001b[0m, in \u001b[0;36m_check_for_bad_pandas_dtypes\u001b[1;34m(pandas_dtypes_series)\u001b[0m\n\u001b[0;32m    799\u001b[0m bad_pandas_dtypes \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpandas_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m column_name, pandas_dtype \u001b[38;5;129;01min\u001b[39;00m pandas_dtypes_series\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_allowed_numpy_dtype(pandas_dtype\u001b[38;5;241m.\u001b[39mtype)\n\u001b[0;32m    803\u001b[0m ]\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bad_pandas_dtypes:\n\u001b[1;32m--> 805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas dtypes must be int, float or bool.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFields with bad pandas dtypes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(bad_pandas_dtypes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    807\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: theater_id_str: object"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "print(\"\\n--- Step 19: Performance Evaluation with SMAPE ---\")\n",
    "\n",
    "# Function to calculate SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    # Handle the case where both true and predicted are zero\n",
    "    # We define 0/0 as 0, so we replace denominator zeros with 1 to avoid division by zero error\n",
    "    return np.mean(numerator / np.where(denominator == 0, 1, denominator)) * 100\n",
    "\n",
    "# Use TimeSeriesSplit for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "smape_scores = []\n",
    "rmse_scores = []\n",
    "fold = 1\n",
    "\n",
    "print(\"Running 3-fold time-series cross-validation to estimate SMAPE...\")\n",
    "\n",
    "for train_index, val_index in tscv.split(X):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # Train model using best parameters\n",
    "    model_eval = lgb.LGBMRegressor(**gscv.best_params_)\n",
    "    model_eval.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on validation set\n",
    "    val_preds = model_eval.predict(X_val)\n",
    "    val_preds[val_preds < 0] = 0\n",
    "\n",
    "    # Calculate scores\n",
    "    fold_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "    fold_smape = smape(y_val.values, val_preds)\n",
    "    rmse_scores.append(fold_rmse)\n",
    "    smape_scores.append(fold_smape)\n",
    "\n",
    "    print(f\"Fold {fold}: RMSE = {fold_rmse:.4f}, SMAPE = {fold_smape:.4f}%\")\n",
    "    fold += 1\n",
    "\n",
    "print(\"\\n--- Cross-Validation Summary ---\")\n",
    "print(f\"Average RMSE: {np.mean(rmse_scores):.4f}\")\n",
    "print(f\"Average SMAPE: {np.mean(smape_scores):.4f}%\")\n",
    "print(\"SMAPE gives an estimate of the model's average percentage error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8856c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "theatre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
